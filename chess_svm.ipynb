{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d263b1-844d-4df0-98bd-d314a7461249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc1758b-ab3a-431e-85bd-ec4820a68050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/giangto/Documents/umn/csci5525/5525Chess\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "current = os.getcwd()\n",
    "data_folder = os.path.join(current, \"normalized_images\")\n",
    "chess_types_folders = glob.glob(os.path.join(data_folder, \"*\"))\n",
    "# print(chess_types_folders)\n",
    "pieces_info = []\n",
    "labels = {\"King\": 1, \"Knight\":2, \"Bishop\":3, \"Rook\":4, \"Pawn\":5, \"Queen\":6}\n",
    "for chess_types in chess_types_folders:\n",
    "    pieces = glob.glob(f'{chess_types}/*')\n",
    "    # print(pieces)\n",
    "    type = chess_types.split(\"/\")[-1]\n",
    "    for piece in pieces:\n",
    "        p = {\"normalized_img\": np.load(piece).reshape(-1), \"label\": labels[type]}\n",
    "        pieces_info.append(p)\n",
    "chess_df = pd.DataFrame(pieces_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa9177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      normalized_img  label\n",
      "0  [0.2196078431372549, 0.2235294117647059, 0.227...      3\n",
      "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...      3\n",
      "2  [0.8666666666666667, 0.8666666666666667, 0.866...      3\n",
      "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...      3\n",
      "4  [0.6862745098039216, 0.6823529411764706, 0.690...      3\n"
     ]
    }
   ],
   "source": [
    "print(chess_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c292719e-7e69-41ba-8a3e-f1dcc6fbd39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_df.iloc[0:5]['normalized_img'].shape #dim of 1 single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cc4226-bb47-4d16-8bff-fdcebad37664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21960784, 0.22352941, 0.22745098, ..., 0.76078431, 0.76078431,\n",
       "       0.76078431], shape=(50176,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_df.iloc[0]['normalized_img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f945153b-37fb-4477-ae87-09af1eeec8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "X, y = np.array(chess_df['normalized_img']), np.array(chess_df['label'])\n",
    "X = np.array([x for x in X])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc0472e-651b-49c3-8eba-d58f828fb7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01: (avg train error, avg validation error) = (0.0000, 0.0000)\n",
      "C = 0.1: (avg train error, avg validation error) = (0.0000, 0.0000)\n",
      "C = 1.0: (avg train error, avg validation error) = (0.0000, 0.0000)\n",
      "C = 10: (avg train error, avg validation error) = (0.0000, 0.0000)\n",
      "C = 100: (avg train error, avg validation error) = (0.0000, 0.0000)\n"
     ]
    }
   ],
   "source": [
    "#Selecting best C for Linear SVM Model\n",
    "C_values = [0.01, 0.1, 1.0, 10, 100]\n",
    "cv_val_avg_score = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for C in C_values:\n",
    "\ttrain_errors = []\n",
    "\tval_errors = []\n",
    "\tfor train_idx, val_idx in skf.split(X_train, y_train):\n",
    "\t\tX_train_cv, X_val = X_train[train_idx], X_train[val_idx]\n",
    "\t\ty_train_cv, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\t\tlin_svm = SVC(kernel='linear',C=C)\n",
    "\t\tlin_svm.fit(X_train, y_train)\n",
    "\t\t# validation error\n",
    "\t\ty_val_pred = lin_svm.predict(X_val)\n",
    "\t\tval_error = 1 - accuracy_score(y_val, y_val_pred)\n",
    "\t\tval_errors.append(val_error)\n",
    "\n",
    "\t\t# train error\n",
    "\t\ty_train_pred = lin_svm.predict(X_train_cv)\n",
    "\t\ttrain_error = 1 - accuracy_score(y_train_cv, y_train_pred)\n",
    "\t\ttrain_errors.append(train_error)\n",
    "\n",
    "\tavg_val_error = np.mean(val_errors)\n",
    "\tavg_train_error = np.mean(train_errors)\n",
    "\n",
    "\t# Report validation, test for each C\n",
    "\tprint(f\"C = {C}: (avg train error, avg validation error) = ({avg_train_error:.4f}, {avg_val_error:.4f})\")\n",
    "\n",
    "\tcv_val_avg_score.append(avg_val_error)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595bdf49-7f0f-4523-adf1-450fcc8b9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C = 0.01, with lowest avg test error: 0.0000\n",
      "Final Test Error: 0.7938\n",
      "Linear SVM Performance Metrics with C=0.01:\n",
      "Accuracy: 0.2062\n",
      "Precision: 0.2074\n",
      "Recall: 0.2062\n",
      "F1 Score: 0.2036\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 5  4  8  9  5  5]\n",
      " [ 8  6  7 15  7  2]\n",
      " [ 9  3 15  4  5  6]\n",
      " [ 3  8  8  7  4  8]\n",
      " [ 7 10 10  9 15  1]\n",
      " [ 6 12  8  4  9  5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.13      0.14      0.14        36\n",
      "           2       0.14      0.13      0.14        45\n",
      "           3       0.27      0.36      0.31        42\n",
      "           4       0.15      0.18      0.16        38\n",
      "           5       0.33      0.29      0.31        52\n",
      "           6       0.19      0.11      0.14        44\n",
      "\n",
      "    accuracy                           0.21       257\n",
      "   macro avg       0.20      0.20      0.20       257\n",
      "weighted avg       0.21      0.21      0.20       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit linear model on best C\n",
    "lowest_val_error_index = np.argmin(cv_val_avg_score)\n",
    "C_chosen = C_values[lowest_val_error_index]\n",
    "\n",
    "print(f\"Best C = {C_chosen}, with lowest avg test error: {cv_val_avg_score[lowest_val_error_index]:.4f}\")\n",
    "\n",
    "\n",
    "final_model = SVC(kernel='linear',C=C_chosen)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Use 'macro' or 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Test Error: {final_test_error:.4f}\")\n",
    "print(f\"Linear SVM Performance Metrics with C={C_chosen}:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd9167d-5723-46c7-a81b-92988a237983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01: (avg train error, avg validation error) = (0.8097, 0.8097)\n",
      "C = 0.1: (avg train error, avg validation error) = (0.8097, 0.8097)\n",
      "C = 1.0: (avg train error, avg validation error) = (0.3205, 0.3206)\n",
      "C = 10: (avg train error, avg validation error) = (0.0351, 0.0351)\n",
      "C = 100: (avg train error, avg validation error) = (0.0000, 0.0000)\n"
     ]
    }
   ],
   "source": [
    "#Selecting best C for RBF SVM Model (gamma=auto)\n",
    "C_values = [0.01, 0.1, 1.0, 10, 100]\n",
    "cv_val_avg_score = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for C in C_values:\n",
    "\ttrain_errors = []\n",
    "\tval_errors = []\n",
    "\tfor train_idx, val_idx in skf.split(X_train, y_train):\n",
    "\t\tX_train_cv, X_val = X_train[train_idx], X_train[val_idx]\n",
    "\t\ty_train_cv, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\t\tauto_svm = SVC(kernel='rbf',C=C, gamma='auto')\n",
    "\t\tauto_svm.fit(X_train, y_train)\n",
    "\t\t# validation error\n",
    "\t\ty_val_pred = auto_svm.predict(X_val)\n",
    "\t\tval_error = 1 - accuracy_score(y_val, y_val_pred)\n",
    "\t\tval_errors.append(val_error)\n",
    "\n",
    "\t\t# train error\n",
    "\t\ty_train_pred = auto_svm.predict(X_train_cv)\n",
    "\t\ttrain_error = 1 - accuracy_score(y_train_cv, y_train_pred)\n",
    "\t\ttrain_errors.append(train_error)\n",
    "\n",
    "\tavg_val_error = np.mean(val_errors)\n",
    "\tavg_train_error = np.mean(train_errors)\n",
    "\n",
    "\t# Report validation, test for each C\n",
    "\tprint(f\"C = {C}: (avg train error, avg validation error) = ({avg_train_error:.4f}, {avg_val_error:.4f})\")\n",
    "\n",
    "\tcv_val_avg_score.append(avg_val_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1db16077-645f-4633-9677-8c983799bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C = 100, with lowest avg test error: 0.0000\n",
      "Final Test Error: 0.7237\n",
      "RBF SVM Performance Metrics with C=100, gamma='auto':\n",
      "Accuracy: 0.2763\n",
      "Precision: 0.2732\n",
      "Recall: 0.2763\n",
      "F1 Score: 0.2727\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4  3  9  3  6 11]\n",
      " [ 3 20  9  6  3  4]\n",
      " [10  4  9  9  4  6]\n",
      " [ 6  7  5 16  4  0]\n",
      " [ 5  8  8  7 18  6]\n",
      " [ 7  4 13  6 10  4]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      0.11      0.11        36\n",
      "           2       0.43      0.44      0.44        45\n",
      "           3       0.17      0.21      0.19        42\n",
      "           4       0.34      0.42      0.38        38\n",
      "           5       0.40      0.35      0.37        52\n",
      "           6       0.13      0.09      0.11        44\n",
      "\n",
      "    accuracy                           0.28       257\n",
      "   macro avg       0.26      0.27      0.27       257\n",
      "weighted avg       0.27      0.28      0.27       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit RBF model on best C (gamma='auto')\n",
    "lowest_val_error_index = np.argmin(cv_val_avg_score)\n",
    "C_chosen = C_values[lowest_val_error_index]\n",
    "\n",
    "print(f\"Best C = {C_chosen}, with lowest avg test error: {cv_val_avg_score[lowest_val_error_index]:.4f}\")\n",
    "\n",
    "\n",
    "final_model = SVC(kernel='rbf',C=C_chosen, gamma='auto')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Use 'macro' or 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Test Error: {final_test_error:.4f}\")\n",
    "print(f\"RBF SVM Performance Metrics with C={C_chosen}, gamma='auto':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7d2fd07-3c95-42b3-93aa-34ae4462bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01: (avg train error, avg validation error) = (0.8097, 0.8097)\n",
      "C = 0.1: (avg train error, avg validation error) = (0.8097, 0.8097)\n",
      "C = 1.0: (avg train error, avg validation error) = (0.3205, 0.3206)\n",
      "C = 10: (avg train error, avg validation error) = (0.0351, 0.0351)\n",
      "C = 100: (avg train error, avg validation error) = (0.0000, 0.0000)\n"
     ]
    }
   ],
   "source": [
    "#Selecting best C for RBF SVM Model (gamma=scale)\n",
    "C_values = [0.01, 0.1, 1.0, 10, 100]\n",
    "cv_val_avg_score = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for C in C_values:\n",
    "\ttrain_errors = []\n",
    "\tval_errors = []\n",
    "\tfor train_idx, val_idx in skf.split(X_train, y_train):\n",
    "\t\tX_train_cv, X_val = X_train[train_idx], X_train[val_idx]\n",
    "\t\ty_train_cv, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\t\tscale_svm = SVC(kernel='rbf',C=C, gamma='scale')\n",
    "\t\tscale_svm.fit(X_train, y_train)\n",
    "\t\t# validation error\n",
    "\t\ty_val_pred = scale_svm.predict(X_val)\n",
    "\t\tval_error = 1 - accuracy_score(y_val, y_val_pred)\n",
    "\t\tval_errors.append(val_error)\n",
    "\n",
    "\t\t# train error\n",
    "\t\ty_train_pred = scale_svm.predict(X_train_cv)\n",
    "\t\ttrain_error = 1 - accuracy_score(y_train_cv, y_train_pred)\n",
    "\t\ttrain_errors.append(train_error)\n",
    "\n",
    "\tavg_val_error = np.mean(val_errors)\n",
    "\tavg_train_error = np.mean(train_errors)\n",
    "\n",
    "\t# Report validation, test for each C\n",
    "\tprint(f\"C = {C}: (avg train error, avg validation error) = ({avg_train_error:.4f}, {avg_val_error:.4f})\")\n",
    "\n",
    "\tcv_val_avg_score.append(avg_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c0021f-c910-47d2-8bfd-b5335f06d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C = 100, with lowest avg test error: 0.0000\n",
      "Final Test Error: 0.7237\n",
      "RBF SVM Performance Metrics with C=100, gamma='scale':\n",
      "Accuracy: 0.2763\n",
      "Precision: 0.2732\n",
      "Recall: 0.2763\n",
      "F1 Score: 0.2727\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4  3  9  3  6 11]\n",
      " [ 3 20  9  6  3  4]\n",
      " [10  4  9  9  4  6]\n",
      " [ 6  7  5 16  4  0]\n",
      " [ 5  8  8  7 18  6]\n",
      " [ 7  4 13  6 10  4]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      0.11      0.11        36\n",
      "           2       0.43      0.44      0.44        45\n",
      "           3       0.17      0.21      0.19        42\n",
      "           4       0.34      0.42      0.38        38\n",
      "           5       0.40      0.35      0.37        52\n",
      "           6       0.13      0.09      0.11        44\n",
      "\n",
      "    accuracy                           0.28       257\n",
      "   macro avg       0.26      0.27      0.27       257\n",
      "weighted avg       0.27      0.28      0.27       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit RBF model on best C (gamma='scale')\n",
    "lowest_val_error_index = np.argmin(cv_val_avg_score)\n",
    "C_chosen = C_values[lowest_val_error_index]\n",
    "\n",
    "print(f\"Best C = {C_chosen}, with lowest avg test error: {cv_val_avg_score[lowest_val_error_index]:.4f}\")\n",
    "\n",
    "\n",
    "final_model = SVC(kernel='rbf',C=C_chosen, gamma='scale')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Use 'macro' or 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Test Error: {final_test_error:.4f}\")\n",
    "print(f\"RBF SVM Performance Metrics with C={C_chosen}, gamma='scale':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624f02a",
   "metadata": {},
   "source": [
    "RBF SVM with gamma = scale and gamma = auto performs comparably okay \n",
    "RBF SVM performs a lot more better with Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c2824a-ec66-4c91-8735-a2137dcfdb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement own version of SVM and tweak something?????\n",
    "#DIY SVM\n",
    "import numpy as np\n",
    "\n",
    "class MultiClassSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, gamma=0.1, C=1.0):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "        self.models = {}  # One binary SVM for each class\n",
    "\n",
    "    def rbf_kernel(self, x, y):\n",
    "        return np.exp(-self.gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for cls in self.classes:\n",
    "            # Create binary labels for one-vs-rest\n",
    "            y_binary = np.where(y == cls, 1, -1)\n",
    "            svm = SVM(self.lr, self.lambda_param, self.n_iters, self.gamma, self.C)\n",
    "            svm.fit(X, y_binary)\n",
    "            self.models[cls] = svm\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = {}\n",
    "        for cls, svm in self.models.items():\n",
    "            scores[cls] = svm.predict_decision_scores(X)  # Decision scores from each SVM\n",
    "        # Return the class with the highest decision score\n",
    "        return np.array([max(scores, key=lambda k: scores[k][i]) for i in range(len(X))])\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, gamma=0.1, C=1.0):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        self.support_vectors = None\n",
    "        self.support_labels = None\n",
    "\n",
    "    def rbf_kernel(self, x, y):\n",
    "        return np.exp(-self.gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y = np.where(y <= 0, -1, 1)  # Convert labels to -1 and 1\n",
    "\n",
    "        # Initialize alpha (Lagrange multipliers)\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent for alpha optimization\n",
    "        for _ in range(self.n_iters):\n",
    "            for i in range(n_samples):\n",
    "                # Calculate decision function using the RBF kernel\n",
    "                decision = 0\n",
    "                for j in range(n_samples):\n",
    "                    decision += self.alpha[j] * y[j] * self.rbf_kernel(X[i], X[j])\n",
    "                decision += self.b\n",
    "\n",
    "                # Update alpha and b if constraint is violated\n",
    "                if y[i] * decision < 1:\n",
    "                    self.alpha[i] += self.lr * (1 - y[i] * decision)\n",
    "\n",
    "                    # Clip alpha to stay within [0, C]\n",
    "                    self.alpha[i] = np.clip(self.alpha[i], 0, self.C)\n",
    "\n",
    "                    self.b += self.lr * y[i]\n",
    "\n",
    "        # Store support vectors (non-zero alpha values)\n",
    "        support_vector_indices = self.alpha > 1e-5\n",
    "        self.support_vectors = X[support_vector_indices]\n",
    "        self.support_labels = y[support_vector_indices]\n",
    "        self.alpha = self.alpha[support_vector_indices]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            decision = 0\n",
    "            for alpha, sv, sv_label in zip(self.alpha, self.support_vectors, self.support_labels):\n",
    "                decision += alpha * sv_label * self.rbf_kernel(x, sv)\n",
    "            decision += self.b\n",
    "            predictions.append(np.sign(decision))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_decision_scores(self, X):\n",
    "        scores = []\n",
    "        for x in X:\n",
    "            decision = 0\n",
    "            for alpha, sv, sv_label in zip(self.alpha, self.support_vectors, self.support_labels):\n",
    "                decision += alpha * sv_label * self.rbf_kernel(x, sv)\n",
    "            decision += self.b\n",
    "            scores.append(decision)\n",
    "        return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f50739",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m diy_svm \u001b[38;5;241m=\u001b[39m MultiClassSVM()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdiy_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m diy_svm\u001b[38;5;241m.\u001b[39mpredict(y_test)\n\u001b[1;32m      5\u001b[0m final_test_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m accuracy_score(y_test, y_test_pred)\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mMultiClassSVM.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m y_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m svm \u001b[38;5;241m=\u001b[39m SVM(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_param, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC)\n\u001b[0;32m---> 23\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m=\u001b[39m svm\n",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m, in \u001b[0;36mSVM.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     60\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m---> 62\u001b[0m     decision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[j] \u001b[38;5;241m*\u001b[39m y[j] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrbf_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m decision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Update alpha and b if constraint is violated\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mSVM.rbf_kernel\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrbf_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diy_svm = MultiClassSVM()\n",
    "diy_svm.fit(X_train, y_train)\n",
    "y_test_pred = diy_svm.predict(y_test)\n",
    "\n",
    "final_test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Use 'macro' or 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Test Error: {final_test_error:.4f}\")\n",
    "print(f\"RBF SVM Performance Metrics with C={C_chosen}, gamma='scale':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
